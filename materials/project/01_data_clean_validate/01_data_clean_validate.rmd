---
title: "01_Data Clean and Validate"
description: "Scheduled document to clean and validate data, then write output to a pin"
output:
  html_document:
    toc: yes
    toc_float: yes
    code_folding: hide
resource_files:
- "email-01_data_integrity_alert.Rmd"
- "email-01_data_summary.Rmd"
---
 
# Load packages
```{r setup, echo=FALSE, warning=FALSE, message=FALSE}

library(tidyverse)
library(janitor)
library(DBI)
library(odbc)
library(RPostgres)
library(pins)
library(pointblank)
library(blastula)
library(glue)
library(gt)
library(arrow) # we might drop this if we decide not to write the pin in arrow format



```


# Overview

1. Pull data from database (`inspections_raw`)
1. Filter inspection data to only types we are interested in (`inspections_filtered`)
1. Validate and pass only passing data (`inspections_validated`)
1. Run a few transforms, mutations, cleanups and finalize dataset processing (`inspections_processed`)
1. Write pins

# Pull data from Database

```{r DB-connection}


con <- dbConnect(RPostgres::Postgres(), 
                 host = Sys.getenv("DB_HOST"), 
                 port = "5432", 
                 dbname = "r_workshop", 
                 user = Sys.getenv("DB_USER"), 
                 password = Sys.getenv("DB_PASSWORD"))


```

```{r Pull-raw-data}

# DATASET 1: Food inspections data, with some time logging for reference
inspections_raw <- 
  tbl(con, "inspections_raw") |> 
  collect() |> 
  # convert inspection_date to date format
  mutate(inspection_date = lubridate::mdy(inspection_date)) 


# DATASET 2: Business data
bus_data_raw <- tbl(con, "bus_data_raw") |>
  # Filter only relevant facility types. Do this before we collect into local memory
  filter(license_description %in% c(
                                    "RETAIL FOOD - SEASONAL LAKEFRONT FOOD ESTABLISHMENT",
                                    "RETAIL FOOD ESTABLISHMENT",
                                    "WRIGLEY FIELD")) |> 
  collect() |> 
  mutate(license_term_expiration_date = lubridate::mdy(license_term_expiration_date)) |> 
  # Add column to determine if license is still valid
  mutate(license_valid = case_when(license_term_expiration_date > lubridate::today() ~ "TRUE", .default = "FALSE")) 

dbDisconnect(con)
```

# Perform inspections data filtering and cleaning.

```{r Thin-out-inspections-data-pre-validation}


inspections_filtered <- 
  inspections_raw |> 
  # Clean facility_type
  mutate(facility_type = case_when(
    grepl("RE?STA?U?R?A?N?T?", `facility_type`) ~ "RESTAURANT",
    grepl("GROCERY", facility_type) ~ "GROCERY STORE", 
    grepl("BA?KE?R?Y?", `facility_type`) ~ "BAKERY",
    grepl("COFFEE *SHOP", `facility_type`) ~ "COFFEE SHOP", 
    grepl("WRIGLEY", `facility_type`) ~ "WRIGLEY FIELD ROOFTOP",
    .default = paste("OTHER")
    )) |> 
  
  # filter out inspections not at a restaurant, grocery store, bakery, coffee shop, or Wrigley
  filter(facility_type != "OTHER") |> 
  
   # Filter for only Pass, Fail, Pass with conditions
  filter(results %in% c("PASS", "FAIL", "PASS W/ CONDITIONS")) 


#Now let's also join in the reference business data table so we have a "source of truth" DBA name to check against.
# inspections_step2 <- 
#   inspections_filtered |> 
#   left_join(select(bus_data_ref, license_number, doing_business_as_name), by=join_by(license_number)) |>
#   rename(ref_dba_name = doing_business_as_name) 

# Finally, add a column with the Jaro ("jw") string distance to help us determine how similar or dissimilar the inspectors' inputted dba name is to the reference. A distance of 0 indicates identical; a 1 indicates completely dissimilar
# inspections_processed <- tidystringdist::tidy_stringdist(inspections_step2,v1=dba_name, v2=ref_dba_name, method="jw") 


```

# Inspections data validation
Perform data validation. The first validation just confirms the overall size and format of the data frame. If the validation fails here, a warning is thrown and the validation will not proceed.
```{r data-fram-size-and-format-validation}


# Define a column schema so we can check inputted data is as expected
# Fun fact, also use .tbl argument in col_schema to compare the dataframe to an ideal table.
# troubleshooting, if this fails, look at the x_list$col_types and $col_names to see the discrepancy
schema_inspections <- col_schema(inspection_id = "numeric",
                               dba_name = "character",
                               aka_name = "character",
                               license_number = "numeric",
                               facility_type = "character",
                               risk = "character",
                               address = "character",
                               city = "character",
                               state = "character",
                               zip = "numeric",
                               inspection_date = "Date",
                               inspection_type = "character",
                               results = "character",
                               violations = "character",
                               latitude = "numeric",
                               longitude = "numeric",
                               location = "character"
                               )


#### VALIDATION 1: Data set integrity validations. All of these trigger a warning under fail conditions.
agent_df_integrity <- 
  create_agent(inspections_filtered, label = "Inital validation of the inspections data set to confirm overall schema and size. If there are issues with this validation, further processing stops and an alert is triggered.") |> 
  # verify column schema 
  # troubleshooting, if this fails, look at the x_list$col_types and $col_names to see the discrepancy
  col_schema_match(schema_inspections, 
                   label = "Is the column schema as expected?", 
                   actions = action_levels(warn_at = 1)) |> 
  #Check that expected columns exist. We make a table in the preconditions using a table transform that is made up of the column names of our inspections table. Then compare those values to the set of schema_inspection names.
  col_vals_in_set(columns = value, 
                  set = names(schema_inspections), 
                  preconditions = ~. %>% tt_tbl_colnames, 
                  label = "Are the expected columns in the data set?", 
                  actions = action_levels(warn_at = 0.01) ) |> 
  # verify there are A LOT of rows of data to be sure import didn't mess up. 
  col_vals_gte(columns = n, 
               value = 100000L, # an arbitrary high-ish number
               preconditions = ~. %>% tally,
               label = "Are there more than 100k rows in the data?", 
               actions = action_levels(warn_at = 1)) |>
  interrogate()
agent_df_integrity

alert_me <- !(all_passed(agent_df_integrity))
```

```{r send-email-alert-for-validation-fail-and-stop-processing-this-file, eval=alert_me, results="asis"}


# note, `results: asis` to make the email html render

if (alert_me == TRUE){
  # send an email alert
  render_connect_email(input = "email-01_data_integrity_alert.Rmd") |> 
    attach_connect_email(subject = "‚ö†Ô∏èInspections Project: 
                         Validation of Inspections dataset found integrity issue")
  
  # logging
  glue("Report run {blastula::add_readable_time()}")
  glue("Validation of inspection data set integrity failed. An email alert was generated, and is shown below. {email_alert$html_html}")
  
  # stop processing this file
  knitr::knit_exit()
} 

```

# Data validation
As long as the the validation on Data Frame integrity passes, the following additional validations will run
```{r data-integrity-validation}


#### VALIDATION 2: Data integrity validations. Failing rows from this validation will be stripped out. We can monitor these over time if we create a multi_agent too.
agent_main <-  
  create_agent(inspections_filtered, label = "Validation of inspections data") |> 
  # No null values
  col_vals_not_null(columns = inspection_id) |> 
  col_vals_not_null(columns = dba_name) |> 
  col_vals_not_null(columns = address) |> 
  col_vals_not_null(columns = latitude) |>
  col_vals_not_null(columns = longitude) |>
  col_vals_not_null(columns = inspection_date) |> 
  col_vals_not_null(columns = inspection_type) |> 
  col_vals_not_null(columns = results) |> 
  # verify inspection date is valid
  col_vals_lte(columns = inspection_date, today(),
               label = "Is inspection date in the past?") |> 
  # verify Inspection ID is unique
  rows_distinct(columns = vars(inspection_id), 
                label = "Is the Inspection ID unique?", 
                actions = action_levels(warn_at = 0.001)) |> 
  # verify inspection ID is valid
  col_vals_between(columns = inspection_id, 1000, 99999999, 
                   label = "Is the Inspection ID a valid entry?") |>  
  # verify license number is valid
  col_vals_between(columns = license_number, 1, 99999999, 
                   label = "Is the License Number a valid entry?") |>
  # verify lat and long bounds (set na_pass = TRUE to ignore NAs)
  col_vals_between(columns = latitude, left = 41.5001, right = 42.3772, na_pass = TRUE) |> 
  col_vals_between(columns = longitude, left = -88.2959, right = -87.316, na_pass = TRUE) |> 
  # verify w col_vals_within_spec for postal_code / aka zip
  col_vals_within_spec(columns = zip, spec = "postal[USA]", na_pass = TRUE) |> 


  # Is the DBA inconsistent with that listed on the Business License?
  # col_vals_lte(columns = jw, 
  #              value = 0.34, 
  #              label = "Dissimilarity measure between reported DBA Name and licensed DBA Name") |> 
  interrogate() 

agent_main

```

# Sunder the data to remove failed rows
```{r sunder-data}


inspections_validated <- get_sundered_data(agent_main, type = "pass")
inspections_fail_validation <- get_sundered_data(agent_main, type = "fail")

```



# Final data transformation and processing

```{r transform-data-to-processed-form}


inspections_processed <- 
  inspections_validated |>
  
  # round the lat / long to 6 decimal places 
  mutate(latitude = round(latitude, 6)) |> 
  mutate(longitude = round(longitude, 6)) |> 
  mutate(location = paste0("(", latitude, ", ",longitude,")")) |> 
  
  # Convert "Pass w/ Conditions" to Fail (since it is a failure, but was corrected
  #  before the food inspector left)
  mutate(results = if_else(results == "PASS W/ CONDITIONS", "FAIL", results)) |> 
  
  # Refactor risk
  mutate(risk = case_when(
    risk == "RISK 1 (HIGH)" ~ 1,
    risk == "RISK 2 (MEDIUM)" ~ 2,
    risk == "RISK 3 (LOW)" ~ 3
  )) |> 
  # Extract violations (v_list) and total violation numbers (v_num)
  rowwise() |> 
  mutate(v_list = case_when(
    is.na(violations) ~ NA,
    !is.na(violations) ~ str_extract_all(violations, 
                                         "(^\\d{1,2}|\\|\\s*\\d{1,2})"))) |> 
  mutate(v_list = case_when(
    is.null(v_list) ~ NA,
    !is.null(v_list) ~ list(str_replace_all(v_list, "\\|\\s*", "")))) |> 
  mutate(v_num = length(v_list)) |> 

  # Detect number of critical/serious violations (#'s 1 to 29)
  mutate(v_num_cs = sum(1:29 %in% unlist(v_list))) |> 

  # Calculate cumsum violation number
  group_by(license_number) |> 
  arrange(inspection_date) |> 
  mutate(v_total = sum(v_num)) |> 
  mutate(v_cumsum = cumsum(v_num)) |> 
  mutate(v_cumsum_cs = cumsum(v_num_cs)) |> 
  ungroup(license_number)  
```


# Summarize info
Report on summary info. How many new inspections happened since last update?  How many total inspections?
```{r summary-info}


board <- board_connect()
user_name <- "katie.masiello"

last_inspection <- board |> pin_read(paste0(user_name,"/inspections_processed"))
last_inspection_date <- last_inspection |> 
  arrange(desc(inspection_date)) |> 
  slice(1) |> 
  pull(inspection_date)


new_inspections <- inspections_processed |> 
  filter(!inspection_id %in% last_inspection$inspection_id) |>
  select(inspection_date, dba_name, facility_type, results, v_num, v_num_cs)

new_inspections |> gt() |> 
  tab_header(
    title = md(glue("**New Inspections Data Added Since {last_inspection_date}**")),
    subtitle = md("Inspections data for Restaurant, Grocery Store, Coffee Shop, Bakery, or Wrigley Field Rooftop ‚öæ")) |>
  tab_source_note(source_note = md(glue("**New records added: {nrow(new_inspections)} <br> Total inspection records: {nrow(inspections_processed)}**"))) |> 
  opt_interactive()


summary_email <- render_connect_email(input = "email-01_data_summary.Rmd") 

attach_connect_email(summary_email, subject = glue("‚ÑπÔ∏èÔ∏èInspections Project: 
                       üçï{nrow(new_inspections)} records added to dataset"))

## TODO: MAKE DATA DICTIONARY

```


# Make table of potential model inputs

## Restaurants that have an active license, based on business license data

Make and pin a reference table of restaurants that have an active business license. They may not have a corresponding entry in the inspections set. (1 record per license number)

```{r make-reference-table-restaurants-w-active-license}



restaurants_active_license <- 
  bus_data_raw |> 
  # Filter out if license is expired
  filter(license_valid == TRUE) |> 
  group_by(license_number) |> 
  arrange(desc(license_term_expiration_date)) |> 
  # Keep only the most recent entry for a particular license number
  slice(1) |> 
  select(-business_activity) |> 
  ungroup()

active_license_numbers <- restaurants_active_license |> pull(license_number)

```
  
## Restaurants that have inspection history and have a current active license

This data will be our model inputs. (1 record per license number)

```{r create-model-inputs}
#| label: Create model inputs - Active restaurants that have inspection history

model_inputs <- inspections_processed |> 
  filter(license_number %in% active_license_numbers) |> 
  select(dba_name, aka_name, license_number, facility_type, risk, address, city, zip, inspection_date, latitude, longitude, v_cumsum, v_cumsum_cs) |> 
  group_by(license_number) |> 
  arrange(desc(inspection_date)) |> 
  slice(1) |> 
  ungroup() |> 
  mutate(days_since_last_inspection = today() - inspection_date)
  

```



# Pin processed data
```{r pin-processed-data-and-restaurants-w-active-licenses}
#| label: Pin processed data and restaurant reference data

board |> pin_write(inspections_processed,
                   name = paste0(user_name,"/inspections_processed"), 
                   description = "Cleaned, validated, and processed inspection data 
                   for inputting into model.",
                   type="arrow")


board |> pin_write(model_inputs, 
                   name = paste0(user_name,"/model_inputs"),
                   description = "Reference df with the list of restaurants that 
                   can be submitted to the model. 
                   From inspection dataset, reduced to facilities that have an 
                   active license and are either a restaurant, grocery store, 
                   coffee shop, bakery, or wrigley field rooftop ‚öæ")


# prune saved versions to last 28 days
pin_versions_prune(board, paste0(user_name,"/inspections_processed"), days = 28)
pin_versions_prune(board, paste0(user_name,"/model_inputs"), days = 28)

```

# Logging information
```{r logging, results="asis"}


glue("Report run {blastula::add_readable_time()}")

glue("Inspection data was validated and sundered. {nrow(inspections_fail_vaildation} rows that failed validation were removed. Restaurant reference data for model and cleaned, validated and processed inspection data written to pin. An email summary of new records added to dataset was generated, and is shown below. {summary_email$html_html}")


```
