---
title: 03_model_creation
---

## Set up Environment

```{r}
#| label: load-packages
#| output: false
#| warning: false
#| message: false

library(tidymodels) # For modeling
library(tidyverse)  # For data analysis
library(embed)      # Extending recipes 
library(janitor)    # For data cleaning
library(vetiver)    # ML ops

tidymodels_prefer() # for handling package conflicts
```

## Read in Data

```{r}
#| label: read-data
#| cache: true

# For testing purposes, I'll only read in 20,000 rows
# inspections_raw <- readr::read_csv("https://data.cityofchicago.org/api/views/4ijn-s7e5/rows.csv", n_max = 20000, show_col_types = FALSE) |> 
#   mutate(Zip = as.character(Zip))


```

## Clean Data

In this step, we'll clean and transform the data in the following ways:

-   Filter for only `Restaurant`, `Grocery Store`, `School`, and `Bakery` .

-   Filter results for only `Pass` and `Fail`. We also convert `Pass w/ Conditions` to `Fail` since the inspection did in fact fail, but was corrected before the inspector left.

-   Filter for only establishments in Chicago, IL.

-   Remove non-relevant columns including `AKA Name`, and `Address`

-   Extract cumulative number of violations to date (`v_cumsum`). Also extract cumulative number of critical or serious violations to day (`v_cumsum_cs`). Violations 1-29 are considered critical and/or serious.

-   Convert the `Risk` column to numeric; `1` for high risk, `2` for medium, and `3` for low.

```{r}
inspections_clean <- inspections_raw |> 
  # Filter for only certain facility types
  filter(`Facility Type` %in% c("Restaurant", "Grocery Store", "School", "Bakery")) |> 
  
  # Filter for only Pass, Fail, Pass with conditions
  filter(Results %in% c("Pass", "Fail", "Pass w/ Conditions")) |>
  
  # Convert "Pass w/ Conditions" to Fail (since it is a failure, but was corrected
  #  before the food inspector left)
  mutate(Results = if_else(Results == "Pass w/ Conditions", "Fail", Results)) |> 
  
  # Filter for Chicago IL
  filter(State == "IL") |> 
  filter(City %in% c("CHICAGO")) |> 
  
  # Remove non-relevant columns
  select(-`AKA Name`, -Address) |> 
  
  # Extract violations (v_list) and total violation numbers (v_num)
  rowwise() |> 
  mutate(v_list = case_when(
    is.na(Violations) ~ NA,
    !is.na(Violations) ~ str_extract_all(Violations, 
                                         "(^\\d{1,2}|\\|\\s*\\d{1,2})"))) |> 
  mutate(v_list = case_when(
    is.null(v_list) ~ NA,
    !is.null(v_list) ~ list(str_replace_all(v_list, "\\|\\s*", "")))) |> 
  mutate(v_num = length(v_list)) |> 

  # Detect number of critical/serious violations (#'s 1 to 29)
  mutate(v_num_cs = sum(1:29 %in% unlist(v_list))) |> 

  # Calculate cumsum violation number
  group_by(`License #`) |> 
  arrange(`Inspection Date`) |> 
  mutate(v_total = sum(v_num)) |> 
  mutate(v_cumsum = cumsum(v_num)) |> 
  mutate(v_cumsum_cs = cumsum(v_num_cs)) |> 
  ungroup(`License #`) |> 
  
  # Refactor risk
  mutate(Risk = case_when(
    Risk == "Risk 1 (High)" ~ 1,
    Risk == "Risk 2 (Medium)" ~ 2,
    Risk == "Risk 3 (Low)" ~ 3
  )) |> 
  
  # Filter for just (potential) predictors
  select(Results, `DBA Name`, `License #`, `Facility Type`, Risk, Zip, Location, v_cumsum, v_cumsum_cs)
```

We'll also clean the column names with the help of the `janitor` package.

```{r}
#| label: janitor-clean

inspections_clean <- clean_names(inspections_clean)
```

## Split Data

Before modeling, it's important to split the data into a training and testing dataset (0.75/0.25 split). We will also stratify by the `Results` variable to ensure the testing and training datasets have similar percentage of `Pass`/`Fail`.

```{r}
#| label: split-data

set.seed(1234)

# Define split
inspections_split <- initial_split(inspections_clean, strata = results, prop = 0.75)
inspections_split

# Create taining and testing datasets
inspections_train <- training(inspections_split)
inspections_test <- testing(inspections_split)
```

## Create Model Recipe

Before we do any actual modeling, we first create a recipe. A recipe is a description of the steps to be applied to a data set in order to prepare it for modeling. In the below recipe, we will do the following:

-   Define our modeling goal, which is to predict `Results` based on `Facility Type`, `Risk`, `Zip`, `v_cumsum`, and `v_cumsum_cs`.

-   Modeling works better with binary predictors (1 or 0). Since `Facility Type` is nominal, we'll widen the column to multiple columns based on the various factors, (e.g., `Restaurant`), and a 1 or 0 as the value, by using the `step_dummy()` function.

-   In the above step, it's easy to convert `Facility Type` to binary since there are only a few unique levels. However, `Zip` has many unique levels. Thus, we can convert `Zip` to a numeric column (or score) by measuring the effect of each zip code on `Results`. In this recipe, we'll use a glm model within the `step_lencode_glm()` function to achieve this.

```{r}
#| label: create-recipe

rec <- recipe(results ~ dba_name + facility_type + risk + zip + v_cumsum + v_cumsum_cs, 
              data = inspections_train) |> 
  # Convert facility type to binary columns
  step_dummy(facility_type) |> 
  # Convert dba_name and zip (which has many levels) to a set of scores
  #  derived from a glm model that estimates the effect
  #  of each Zip code on the outcome. Likelihood encoding.
  step_lencode_glm(zip, outcome = vars(results)) |> 
  step_lencode_glm(dba_name, outcome = vars(results))

rec
```

We are going to use this recipe in a `workflow()` so there is no need to `prep()` or `bake()` the recipe. However, it may be useful to see what the training data will look like after it's put through the recipe, and we can do that below.

```{r}
#| label: view-recipe-output

#bake(prep(rec), new_data = NULL)
```

## Build xgboost model

We first need to define some specifications for the xgboost model. There are lots of them, and we'll insert the `tune()` placeholder for now.

Besides the `trees` specification which we'll set to 1000 (i.e., we'll use 1000 trees, or ensemble, to vote on a final predictions),the only specification we'll manually add is `learn_rate = 0.01`. For boosting, the general idea is that the tuning parameters are tweaked slightly in order to minimize the loss function. The loss function is a method for evaluating how well the algorithm models the dataset. A perfect model would have a loss of 0.

![](https://bradleyboehmke.github.io/HOML/10-gradient-boosting_files/figure-html/gradient-descent-fig-1.png){fig-align="center" width="246"}

During the boosting steps, the model assess the loss function for each combination of parameters and takes **steps** in the direction of the descending gradient until it hits the bottom most point. An important parameter in gradient descent is the size of the steps, which is set by the learning rate. Too big a learning rate and you may miss the minimum. Too small a learning rate, and the model make take forever to reach the minimum.

![](https://bradleyboehmke.github.io/HOML/10-gradient-boosting_files/figure-html/learning-rate-fig-1.png){fig-align="center" width="442"}

Some of the other parameters we'll let `tidymodels` tune for us:

-   `min_n`: how many data points in a node that is required for the node to be split further. Consider this image, where each tree with 10 data points along the top is being split. But if the model had `min_n = 12`, then the model would not split any further and this would be a terminal node.

![](http://blog.hackerearth.com/wp-content/uploads/2016/12/bigd.png){fig-align="center" width="256"}

-   `tree_depth`: How many splits can a tree have.
-   `loss_reduction`: If the model can't split a tree further that reduces the loss function by the value defined here, then it won't split further.
-   `sample_size`: what proportion of the training data will be used for each iteration of the model.
-   `mtry`: Number of randomly selected predictors for each tree.

```{r}
#| label: model-specs

# xgboost model specifications
xgb_spec <- parsnip::boost_tree(
  mode = "classification",
  trees = 1000,
  tree_depth = tune(),
  min_n = tune(),
  loss_reduction = tune(),
  sample_size = tune(),
  mtry = tune(),
  learn_rate = 0.01
) |> 
  set_engine("xgboost")

xgb_spec
```

Now we can create a workflow with these specifications and our recipe:

```{r}
#| label: create-workflow

xgb_wf <- workflow() |> 
  add_recipe(rec) |> 
  add_model(xgb_spec)

xgb_wf
```

Create cross-validation re-samples for tuning our model. We'll make sure there is a equal split of `Pass`/`Fail` results for each fold by stratifying on `results`.

```{r}
#| label: create-cv

set.seed(1234)
inspections_folds <- vfold_cv(inspections_train, strata = results)

inspections_folds
```

Next, we'll create a set of tuning parameters (30) as defined by the specifications above, and assess the performance of each set on the cross-validation re-samples. This step may take a while :)

```{r}
#| label: tune-xgb-model

doParallel::registerDoParallel()

set.seed(1234)
xgb_res <- tune_grid(
  xgb_wf,
  resamples = inspections_folds,
  grid = 30,
  control = control_grid(save_pred = TRUE)
)

xgb_res
```

After each combination of tuning parameters has been assessed, we can view the performance metrics.

```{r}
#| label: view-performance-metrics

collect_metrics(xgb_res)

autoplot(xgb_res)
```

And then we can extract the best performing combination of parameters and show the ROC curve:

```{r}
# Show top combinations
show_best(xgb_res, "roc_auc")

# Save best combination
xgb_best <- select_best(xgb_res)

# View predictions for best combination
xgb_res |> 
  collect_predictions(parameters = xgb_best) |> View()

# Show ROC for best combination
xgb_res |> 
  collect_predictions(parameters = xgb_best) |> 
  roc_curve(results, .pred_Fail) |> 
  mutate(model = "xgb") |> 
  autoplot()
```

Create the final model:

```{r}
#| label: create-final-xbg-model

xgb_final <- xgb_spec |> 
  finalize_model(xgb_best)


```

# Save Model with Vetiver

```{r}
#| label: create-vetiver-model

v <- vetiver_model(
  xgb_final,
  "insepctions-xgboost-model"
)

```
