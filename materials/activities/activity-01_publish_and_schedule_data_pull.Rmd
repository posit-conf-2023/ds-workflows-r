---
title: "activity-01_publish_and_schedule_data_pull"
description: This document pulls our raw data and saves it to the database. Publish this to Connect and schedule it to run at an appropriate interval
output:
  html_document:
    toc: yes
    toc_float: yes
    code_folding: hide
editor_options:
  chunk_output_type: console
---
# Activity Notes

This notebook is a simplified version of the deployed notebook used for the project solution. This notebook is stripped down to only pull the inspection raw data and write a small subset to the database (business data omitted just to keep things speedier). You may find it interesting to browse the project notebook (`materials/project/00_raw_data_scan_db_write.rmd`) to see additional elements that make for a more robust component in the project pipeline. The project solution has these additional features: 

- A check to ensure that at least 100k rows were written to the database. Failure of this would indicate an issue with the input data source.
- An email alert if the number of rows written is not more than 100k
- A pointblank data scan for reference and exploration
- Ability to set the document to "TEST MODE" which will speed up processing by pulling a smaller test mode raw data set, avoids writes to the database, and generates the alert email for inspection

```{r setup, echo=FALSE, warning=FALSE, message=FALSE}

library(tidyverse)
library(janitor)
library(DBI)
library(odbc)
library(RPostgres)
library(glue)

```

# Activity prework - Download data from Chicago Data Portal
🔄 Task

- Download the raw inspections data from the Chicago Data Portal 

✅ Solution


```{r Download-data-from-Chicago-Data-Portal}


start_time <- Sys.time()
inspections_raw <- readr::read_csv("https://data.cityofchicago.org/api/views/4ijn-s7e5/rows.csv") |> 
  #clean column names so they are postgres-compliant (no special characters or spaces) and less annoying to type
  clean_names() |> 
  #convert data to uppercase for uniformity
  mutate(across(where(is.character), toupper))
end_time <- Sys.time()
duration <- end_time - start_time
print(paste("ℹ️ Info: Downloading Inspection data took", round(duration[[1]], 2),  units(duration)))


```

# Activity - Load data into databse
🔄 Task

- Pair up with a neighbor. Chose who will use the `odbc` package and who will use `RPostgres
- Write a subset of the inspections data into the database (use a subset for sake of time. A fraction of 1% has been chosen for you to write ~2.5k rows)
- Compare your write times with your neighbor

✅ Solution

<!-- TODO: Change to DSN for easier publishing -->
```{r Write-raw-data-to-database}


# The `RPostgres` package is DBI-compliant and is built specifically for PosgreSQL DB. Performance may be faster (particularly for writing data!) than using the generic odbc package. It's perfectly reasonable to use the odbc package instead, and if we do, if we instead use the generic `odbc::odbc()` backend, the database will appear in the Connections pane. 
con <- dbConnect(RPostgres::Postgres(), 
                 host = Sys.getenv("CONF23_DB_HOST"), 
                 port = "5432", 
                 dbname = "conf23_r", 
                 user = Sys.getenv("CONF23_DB_USER"), 
                 password = Sys.getenv("CONF23_DB_PASSWORD"))

# con_alternative <- dbConnect(odbc::odbc(), 
#                   Driver = "postgresql", 
#                   Server = Sys.getenv("CONF23_DB_HOST"), 
#                   Port = "5432", 
#                   Database = "conf23_r", 
#                   UID = Sys.getenv("CONF23_DB_USER"),    
#                   PWD = Sys.getenv("CONF23_DB_PASSWORD"), 
#                   timeout = 10)

# specify username to append to the database table
my_name <- Sys.getenv("USER")

# Take a random sample of only 1% of the data for the sake of reducing the time for processing (est. ~2.5k rows)
inspections_raw_subset <- inspections_raw |> sample_frac(0.01)

# write inspections raw subset data
start_time <- Sys.time()
dbWriteTable(con, paste0("inspections_raw_subset",my_name), inspections_raw_subset, overwrite = TRUE)
end_time <- Sys.time()
duration <- end_time - start_time
print(paste("ℹ️ Info: Writing raw inspection data to database took", round(duration[[1]], 2),  units(duration)))

# count how many rows were written to the db
insp_db <- tbl(con, paste0("inspections_raw_subset",my_name))
insp_db_rowcount <- insp_db |> tally() |> pull(n) |> as.numeric()


dbDisconnect(con)
```

# Logging information for our scheduled report
```{r logging, echo=FALSE}

glue("Report run {blastula::add_readable_time()}")

glue("{nrow(inspections_raw)} rows of inspection data downloaded. 
     {insp_db_rowcount} rows of inspection data written successfully to database.")


```

# Activity - Publish this document to Connect and schedule it
🔄 Task

- Use push-button publishing to publish this document, with source code, to Connect
- Schedule it to run on a timescale that seems appropriate for the data update cycle

Notes: 
- Recall we specified our database connection details with environment variables. Typically when you publish content to Connect, you will also need to supply the environment variables using the "Vars" pane in Connect. For this workshop, these environment variables have already been stored on the Connect server for you so you won't need to do this step for this activity.
✅ Solution

The Connect User Guide provides instructions for publishing: <https://docs.posit.co/connect/user/publishing/#publishing-general>



