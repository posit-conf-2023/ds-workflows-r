---
title: "00_Raw Data Exploration"
description: "Conduct ad hoc exploration of the data, first exposure to `pointblank`"
toc: true
toc-depth: 4
editor_options: 
  chunk_output_type: console
execute: 
  eval: false
---

This workshop will use data from the City of Chicago Open Data Portal: <https://data.cityofchicago.org>. The following datasets will be used:

1.  üçïFood inspections: <https://data.cityofchicago.org/Health-Human-Services/Food-Inspections/4ijn-s7e5>
2.  üìíBusiness license data: <https://data.cityofchicago.org/Community-Economic-Development/Business-Licenses/r5kz-chrr>

## Setup

Load required packages for importing and exploring the data

```{r}
#| label: Load packages

library(tidyverse)
library(pointblank)

```

## Activity 1 - Read data
üîÑ Task

- Explore the two data sources
- Load the data into tibbles

‚úÖ Solution

The City of Chicago data portal provides access to their datasets via the Socrata Open Data (SODA) API:

> The Socrata Open Data API (SODA) provides programmatic access to this dataset including the ability to filter, query, and aggregate data.
=~
Consulting the documentation provides us with some examples of how to use SODA: <https://dev.socrata.com/foundry/data.cityofchicago.org/4ijn-s7e5>.

There is a R package called `RSocrata` to interact with SODA, however we can also query the SODA API directly and download the full dataset as a `.csv` using `readr::read_csv()`. In our initial testing, the column schemas were better preserved using the latter method as opposed to using the `RSocrata` package.



### Download data from City of Chicago

#### DATASET 1: Food inspections data

```{r}
#| label: Pull raw inspections data

inspections_raw <- 
  readr::read_csv("https://data.cityofchicago.org/api/views/4ijn-s7e5/rows.csv") 
```



#### DATASET 2: Business data

```{r}
#| label: Pull raw business data

bus_data_raw <- 
  readr::read_csv("https://data.cityofchicago.org/api/views/r5kz-chrr/rows.csv") 
```

## Activity 2 - Scan the data

üîÑ Task

Use the `pointblank` package to gather basic information about the Food Inspections data so we can

- Understand what the data is
- Get a sense for how much cleaning is required
- Plan our approach for data validation


‚úÖ Solution

::: callout-tip
## The `pointblank` package

::: columns
::: {.column width="20%"}
![](https://github.com/rstudio/pointblank/raw/main/man/figures/logo.svg){style="padding-right: 15px;"}
:::

::: {.column width="75%"}
<br> `pointblank` provides data quality assessment and metadata reporting for data frames and database tables. 

**Reference**: <https://github.com/rstudio/pointblank>
:::
:::
:::

The `pointblank::scan_data()` function provides a HTML report of the input data to help you understand your data. It contains 6 sections:

-   **Overview (O)**: Table dimensions, duplicate row counts, column types, and reproducibility information
-   **Variables (V)**: A summary for each table variable and further statistics and summaries depending on the variable type
-   **Interactions (I)**: A matrix plot that shows interactions between variables
-   **Correlations (C)**: A set of correlation matrix plots for numerical variables
-   **Missing Values (M)**: A summary figure that shows the degree of missingness across variables
-   **Sample (S)**: A table that provides the head and tail rows of the dataset

The scan can take a little while to run on a large dataset, but you can also omit sections that are not needed. 

First try it out on a small data frame, such as `mtcars`:

```{r}
#| label: Data scan on mtcars

pointblank::scan_data(mtcars)

```

Our Food Inspection data is much larger than mtcars so to save time, and because these sections aren't relevant for this data, we omit "Correlations" and "Interactions."

```{r}
#| label: Data scan on inspections_raw

scan <- pointblank::scan_data(inspections_raw, sections = "OVMS")
scan



```

## Activity 3 - Explore the data scan

üîÑ Task

Explore the data scan and share your observations in slido.  

1.  What is the most common value for `DBA NAME` and `AKA Name` (hint: Toggle details)
2.  Explore the `Facility Type`. Does the number of distinct values surprise you?
3.  What can we the Missing Values diagram tell us about potential data quality issues?

How will this inform our data cleaning and validation?  

‚úÖ Solution

When we inspect the Facility Type, we see there is a lot of variation in how the inspector recorded this data. It certainly doesn't align with the stated possible values in the data documentation.

```{r}
#| label: Explore variation in the Facility Type

# What variations are there for "Restaurant" 
inspections_raw |> 
  filter(grepl("REST", `Facility Type`, ignore.case = TRUE)) |> 
  group_by(`Facility Type`) |> 
  tally() |> 
  arrange(desc(n)) |> print(n=20)

# Or Grocery Store?
inspections_raw |> 
  filter(grepl("GROC", `Facility Type`, ignore.case = TRUE)) |> 
  group_by(`Facility Type`) |> 
  tally() |> 
  arrange(desc(n)) |> print(n=20)

# Bakery
inspections_raw |> 
  filter(grepl("BAKERY", `Facility Type`, ignore.case = TRUE)) |>
  group_by(`Facility Type`) |>
  tally() |> 
  arrange(desc(n)) |> print(n=20)

# Coffee Shop
inspections_raw |> 
  filter(grepl("COFFEE", `Facility Type`, ignore.case = TRUE)) |>
  group_by(`Facility Type`) |>
  tally() |> 
  arrange(desc(n)) |> print(n=20)
```


Some conclusions from exploration: 

- We've got some very messy data, particularly in `DBA NAME` and `FACILITY TYPE`
- We are going to want the `FACILITY TYPE` to be as clean as possible so we can filter out the establishments we don't care about for this analysis (e.g., hotels, caterers, etc.). A grep search seems sufficient for identifying types of interest.
- For our model, we will need the `DBA NAME` to be as clean and consistent as possible, so we should do some work here too. We will plan to join the Business License data with the Inspections data based on License Number so we can have a "source of truth" DBA Name.

<!-- Let's to a first pass thin out the data based on facility_type -->

<!-- ```{r} -->
<!-- #| label: thin out data -->

<!-- # When we inspect the Facility Type, we see there is a lot of variation in how the inspector recorded this data. It certainly doesn't align with the stated possible values in the data documentation. For example:  -->
<!-- inspections_raw |> filter(grepl("REST", facility_type)) |> group_by(facility_type) |> tally() |> arrange(desc(n)) -->
<!-- inspections_raw |> filter(grepl("BAKERY", facility_type)) |> group_by(facility_type) |> tally() |> arrange(desc(n)) -->
<!-- inspections_raw |> filter(grepl("COFFEE", facility_type)) |> group_by(facility_type) |> tally() |> arrange(desc(n)) -->

<!-- # We'd like to work with the following types: Restaurant, Bakery, Coffee Shop. And The Wrigley Field Rooftop sounds like a blast.  We'll filter the Facility Type based on regex to account for abbreviations, mis-spellings, and extra information. This is a coarse filter on the raw data before validation to thin out the dataset.   -->

<!-- inspections_pass1 <-  -->
<!--   inspections_raw |>  -->
<!--   filter( -->
<!--     # Filter for "RESTAURANTS" and variations including "REST", "RSTRNT", etc. -->
<!--     grepl("RE?STA?U?R?A?N?T?", `facility_type`) |  -->
<!--     # OR Filter for "BAKERY" and variations to account for typos or shortenings -->
<!--     grepl("BA?KE?R?Y?", `facility_type`) |  -->
<!--     # OR Filter for "COFFEE SHOP" and variations, but consciously including "SHOP" to avoid carts, roasters, vending machines -->
<!--     grepl("COFFEE *SHOP", `facility_type`) |  -->
<!--     # OR Filter for "WRIGLEY" -->
<!--     grepl("WRIGLEY", `facility_type`) -->
<!--     )  -->

<!-- #Now let's also join in the reference business data table so we have a "source of truth" DBA name to check against. -->
<!-- inspections_pass2 <-  -->
<!--   inspections_pass1 |>  -->
<!--   left_join(select(bus_data_ref, license_number, doing_business_as_name), by=join_by(license_number)) |> -->
<!--   rename(ref_dba_name = doing_business_as_name)  -->

<!-- # Finally, add a column with the Jaro ("jw") string distance to help us determine how similar or dissimilar the inspectors' inputted dba name is to the reference. A distance of 0 indicates identical; a 1 indicates completely dissimilar -->
<!-- inspections <- tidystringdist::tidy_stringdist(inspections_pass2,v1=dba_name, v2=ref_dba_name, method="jw")  -->

<!-- # inspecting the data, we can select a jw value of 0.34 as a reasonable threshold for identifying dissimilar values. Take a look: -->
<!-- hist(inspections$jw, breaks = c(seq(0,1,0.05))) -->
<!-- # these are definitely very different -->
<!-- inspections |> filter(jw > 0.5) |> select(dba_name, ref_dba_name, jw) |> distinct() |> arrange(jw) -->
<!-- # these are generally the same -->
<!-- inspections |> filter(jw < 0.2) |> select(dba_name, ref_dba_name, jw) |> distinct() |> arrange(desc(jw)) -->
<!-- # goldilocks value of 0.34 seems to be about right. -->
<!-- inspections |> filter(jw <0.5 & jw > 0.2) |> select(dba_name, ref_dba_name, jw) |> distinct() |> arrange(jw) -->

<!-- ``` -->

At this point, we have a good sense of the data and can move on from ad hoc exploration into a repeatable workflow üéâ
