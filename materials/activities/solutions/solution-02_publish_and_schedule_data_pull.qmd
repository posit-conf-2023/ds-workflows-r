---
title: "activity-02_publish_and_schedule_data_pull"
description: This document pulls our raw data and saves it to the database. Publish this to Connect and schedule it to run at an appropriate interval
toc: true
toc-depth: 4
editor: visual
editor_options: 
  markdown:
    wrap: 72
    canonical: true
  chunk_output_type: console
execute: 
  eval: false
---

## Activity Notes

This Notebook will simulate a real-life asset that sources raw data and
writes it to a database. We will publish this document to Connect and
schedule it to run at a regular interval, simulating a production
workflow. We'll add informational logs to our notebook that prove useful
when we review our published output and historical renderings on
Connect.

This document is a pared down version of the project notebook
`/materials/project/00_raw_data_scan_db_write/00_raw_data_scan_db_write.rmd`,
which you might find interesting to review on your own.

## Goals

The goals of this activity are to:

-   Gain experience using `DBI` commands interact with a database

-   Learn some general logging tips to make your scheduled reports more
    useful

-   Publish this notebook to Posit Connect and put it on a rendering
    schedule so database updates happen automatically

This will give you experience setting up a repeatable workflow for
populating production data sources.

‚úèÔ∏è There will be placeholders (`____`) in the code cells below that you
will need to fill in!

## Setup

### Load required packages for importing and writing the data

```{r}
#| label: Load packages

library(tidyverse)
library(janitor)
library(DBI)
library(RPostgres)
library(glue)

```

### Download raw data

Just like the last activity, we will download the raw inspections data
from the Chicago Data Portal, but because this will become an automated
script, we will also include logging in our script that will provide
useful information when we review our published and scheduled output.

A useful informational log will be knowing how long the data took to
download. One means for doing this is to surround our download step with
time stamps, following this model:

``` r
start_time <- Sys.time()

#operation that we want to time

end_time <- Sys.time()
duration <- end_time - start_time
print(paste("Operation took", round(duration[[1]], 2),  units(duration)))
```

We incorporate this into the chunk below.

```{r}
#| label: Download raw inspections data from City of Chicago

## It's not necessary to run this chunk during the Activity time if you still have `inspections_raw` in memory from the last Activity. Save yourself some time üòâ

# Start time stamp
start_time <- Sys.time()

inspections_raw <- 
  readr::read_csv(paste0(
    "https://data.cityofchicago.org/api/views/4ijn-s7e5/rows.csv?$$app_token=",
    Sys.getenv("SOCRATA_APP_TOKEN")
    ))

# End time stamp
end_time <- Sys.time()

print(paste("‚ÑπÔ∏è Info: Downloading Inspection data took", round(duration[[1]], 2),  units(duration)))

```

### Prepare the data for database write

We want to do a light reformatting on the data just to prepare it for
the database. Some databases get angry if there are special characters
in the column names. Let's do the following:

-   Use the `janitor` package clean the column names so that they have
    no special characters or spaces, and they are the same case

-   Convert the data to uppercase for uniformity in downstream
    processing

```{r}
#| label: see the special characters and spaces in raw data column names

names(inspections_raw)

```

```{r}
#| label: reformat data


inspections_raw <- inspections_raw |> 
  #clean column names 
  janitor::clean_names() |> 
  #convert data to all uppercase or lowercase for uniformity
  mutate(across(where(is.character), toupper))

head(inspections_raw)
```

Ahh. Much better.

## Task 1 - Subset the data

For the sake of time, we don't want to write all \~260k rows of the
inspections data to the database, so first, we will take a small sample
and append our names to the data frame to provide unique names.

```{r}
#| label: make raw data subset

my_username <- "katie.masiello"

my_df_name <- paste0("inspections_raw_subset_", my_username)

# Sample a small fraction of the data to keep this example database write snappy
inspections_raw_subset <- inspections_raw |> sample_frac(0.01)

inspections_raw_subset
```

## Task 2 - Connect to the database

üîÑ Task

Create a database connection using the database connection details
defined for you by your IT Admin (Workshop Instructors). Avoid
hard-coding in any credentials. We're using environment variables here.

The `RPostgres` package is DBI-compliant and is built specifically for
PosgreSQL databases. Performance may be faster (particularly for writing
data!) than using the generic `odbc` package, and there are more
translations available, meaning more dplyr verbs will be available.

```{r}
#| label: make database connection

# Run this code as-is 
con <- dbConnect(RPostgres::Postgres(), 
                 host = Sys.getenv("CONF23_DB_HOST"), 
                 port = "5432", 
                 dbname = "conf23_r", 
                 user = Sys.getenv("CONF23_DB_USER"), 
                 password = Sys.getenv("CONF23_DB_PASSWORD"))

con

```

## Task 3 - Write to the database

üîÑ Task

-   Use `dbWriteTable` to write `inspections_raw_subset` into the
    database
-   Use the time stamp method from above to provide informational
    logging

```{r}
#| label: database write


# Insert your start time stamp
start_time <- Sys.time()

DBI::dbWriteTable(conn = con, # the connection
                  name = my_df_name, # the name of the table you will create in the DB
                  value = inspections_raw_subset,
                  overwrite = TRUE)


# Insert your end time stamp
end_time <- Sys.time()


print(paste("‚ÑπÔ∏è Info: Writing raw inspection data to database took", round(duration[[1]], 2),  units(duration)))
```

## Task 4 - Verify number of rows written to database

üîÑ Task

As a check, and for more practice with interacting with databases, let's
determine the number of rows written to the database.

-   Define your table with `dplyr::tbl()`

-   Use `dplyr` verbs to determine the row count *without bringing the
    table back into local memory*

First let's find our table in the database.

```{r}
#| label: DBI list tables
#| 
DBI::dbListTables(con)

my_db_table <- dplyr::tbl(con, "inspections_raw_subset_katie.masiello")
my_db_table
```

There are many ways to count database rows. However, if you just view
`my_db_table`, do you see this in the output:

```         
# Source:   table<inspections_raw_subset_katie.masiello> [?? x 17]
```

That's not very informative!

If we pull the database into local memory using `collect()`, we could
see (and count) all the rows, but we want to avoid this for performance
and resource sake. Use a `dplyr` aggregation command, like `tally`,
`count`, or `summarise` to give an answer without pulling the database
into memory.

```{r}
#| label: count tbl rows

# Be sure row_count is a numeric value, not a data frame (hint: use `dplyr::pull()` to extract a column as vector)
tbl_row_count <- my_db_table |> tally() |> pull(n)
tbl_row_count

#Close the database connection 
dbDisconnect(con)

```

## Task 5 - Add logging information to make our scheduled report informative

üîÑ Task

Make your scheduled reports work harder for you! Include information
that you'll find useful to refer back to. Logging can be as basic or
elaborate as you need.

Lets include:

-   `blastula::add_readable_time()` üëà we will come back to `blastula`
    in another activity, but this function is a nice one to add a
    friendly time stamp.

-   A summary the main actions taken in the report, such as how many
    rows of data were downloaded, how many rows of data were written to
    the database

Hint: The `glue` package is helpful for combining text and variables.
It's less work than stringing things together with `paste`. With
`glue::glue()`, the syntax is
`glue("Text with {<r expression or variable>}")`

```{r}
#| label: logging
glue("Report run {blastula::add_readable_time()}")

glue("{nrow(inspections_raw)} rows of inspection data downloaded. 
     {tbl_row_count} rows of inspection data written to database.")


```

## Task 6 - Publish and schedule on Posit Connect

üîÑ Task

-   Connect your account on Posit Connect to Workbench (Tools \> Global
    Options \> Publishing)
-   Use push-button publishing to publish this document, with source
    code, to Connect
-   Schedule it to run on a timescale that seems appropriate for the
    data update cycle

Note: Recall we specified our database connection details with
environment variables. Typically when you publish content to Connect,
you will also need to supply the environment variables using the "Vars"
pane in Connect. For this workshop, these environment variables have
already been stored on the Connect server for you so you won't need to
do this step for this activity.

We'll do this task together as a group.

For reference, the Connect User Guide provides instructions for
publishing:
<https://docs.posit.co/connect/user/publishing/#publishing-general>
