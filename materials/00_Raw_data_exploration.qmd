---
title: "00_Raw Data Exploration"
description: "Conduct ad hoc exploration of the data, first exposure to {`pointblank`}"
editor_options: 
  chunk_output_type: console
---

```{r}
#| label: Load packages

library(tidyverse)
library(janitor)
library(tidystringdist)
```


Pull data from City of Chicago
```{r}
#| label: Pull raw data

# DATASET 1: Food inspections data, with some time logging for rerference
start_time <- Sys.time()
inspections_raw <- readr::read_csv("https://data.cityofchicago.org/api/views/4ijn-s7e5/rows.csv") |> 
  #clean column names so they are postgres-compliant (no special characters or spaces) and less annoying to type
  clean_names() |> 
  #convert data to uppercase for uniformity
  mutate(across(where(is.character), toupper)) 
end_time <- Sys.time()
duration <- end_time - start_time
print(paste("ℹ️ Info: Downloading Inspection data took", round(duration[[1]], 2),  units(duration)))


# DATASET 2: Business data
start_time <- Sys.time()
bus_data_raw <- readr::read_csv("https://data.cityofchicago.org/api/views/r5kz-chrr/rows.csv") |> 
  #clean column names so they are postgres-compliant (no special characters or spaces) and less annoying to type
  clean_names() |> 
  #Drop unnecessary columns. We only want to use this table to cross reference Licence Number to business name, location, 
  select(license_number, 
         legal_name, 
         doing_business_as_name, 
         license_term_expiration_date,
         address, 
         city, 
         state, 
         zip_code, 
         license_description, 
         business_activity,
         latitude, 
         longitude, 
         location) |> 
  #convert data to uppercase for uniformity
  mutate(across(where(is.character), toupper)) 
end_time <- Sys.time()
duration <- end_time - start_time
print(paste("ℹ️ Info: Downloading Business License data took", round(duration[[1]], 2),  units(duration)))


```

```{r}
#| label: DBA reference table 

# make a reference table of DBA Names using the most current business license number to act as the "source of truth" for correct DBA names
bus_data_ref <- 
  bus_data_raw |> 
  # Convert date field from character to date format
  mutate(license_term_expiration_date = lubridate::mdy(license_term_expiration_date)) |> 
  group_by(license_number) |> 
  arrange(desc(license_term_expiration_date)) |> 
  # Keep only the most rencent entry for a particular license number
  slice(1) |> 
  ungroup()


```
  


Inspect the data
```{r}
#| label: Data scan

# The pointblank::scan_data function will give us a lovely overview of the data. It takes a little while to run, particularly the Variables and Interactions sections. See this simple example:
pointblank::scan_data(mtcars)


# Our inspection data is much larger than mtcars so to save time, and because these sections aren't relevant for this data, we omit "Correlations" and "Interactions."  
scan <- pointblank::scan_data(inspections_raw, sections = "OVMS")
scan



```

Explore the data scan.

1. What is the most common value for `dba_name` and `AKA Name` (hint: Toggle details)
1. Explore the `facility_type`. What values should we consider filtering out or preserving?
1. What can we infer about the data from the Missing Values diagram?


Conclusions:
We've got some very messy data, particularly in dba_name and facility_type. We are going to want the facility_type to be as clean as possible so we can filter out the establishments we don't care about for this analysis (e.g., hotels, caterers, etc.) For our model, we will need the dba_name to be as clean and consistent as possible, so we should do some work here too. 


Let's to a first pass thin out the data based on facility_type
```{r}
#| label: thin out data

# When we inspect the Facility Type, we see there is a lot of variation in how the inspector recorded this data. It certainly doesn't align with the stated possible values in the data documentation. For example: 
inspections_raw |> filter(grepl("REST", facility_type)) |> group_by(facility_type) |> tally() |> arrange(desc(n))
inspections_raw |> filter(grepl("BAKERY", facility_type)) |> group_by(facility_type) |> tally() |> arrange(desc(n))
inspections_raw |> filter(grepl("COFFEE", facility_type)) |> group_by(facility_type) |> tally() |> arrange(desc(n))

# We'd like to work with the following types: Restaurant, Bakery, Coffee Shop. And The Wrigley Field Rooftop sounds like a blast.  We'll filter the Facility Type based on regex to account for abbreviations, mis-spellings, and extra information. This is a coarse filter on the raw data before validation to thin out the dataset.  

inspections_pass1 <- 
  inspections_raw |> 
  filter(
    # Filter for "RESTAURANTS" and variations including "REST", "RSTRNT", etc.
    grepl("RE?STA?U?R?A?N?T?", `facility_type`) | 
    # OR Filter for "BAKERY" and variations to account for typos or shortenings
    grepl("BA?KE?R?Y?", `facility_type`) | 
    # OR Filter for "COFFEE SHOP" and variations, but consciously including "SHOP" to avoid carts, roasters, vending machines
    grepl("COFFEE *SHOP", `facility_type`) | 
    # OR Filter for "WRIGLEY"
    grepl("WRIGLEY", `facility_type`)
    ) 

#Now let's also join in the reference business data table so we have a "source of truth" DBA name to check against.
inspections_pass2 <- 
  inspections_pass1 |> 
  left_join(select(bus_data_ref, license_number, doing_business_as_name), by=join_by(license_number)) |>
  rename(ref_dba_name = doing_business_as_name) 

# Finally, add a column with the Jaro ("jw") string distance to help us determine how similar or dissimilar the inspectors' inputted dba name is to the reference. A distance of 0 indicates identical; a 1 indicates completely dissimilar
inspections <- tidystringdist::tidy_stringdist(inspections_pass2,v1=dba_name, v2=ref_dba_name, method="jw") 

# inspecting the data, we can select a jw value of 0.34 as a reasonable threshold for identifying dissimilar values. Take a look:
hist(inspections$jw, breaks = c(seq(0,1,0.05)))
# these are definitely very different
inspections |> filter(jw > 0.5) |> select(dba_name, ref_dba_name, jw) |> distinct() |> arrange(jw)
# these are generally the same
inspections |> filter(jw < 0.2) |> select(dba_name, ref_dba_name, jw) |> distinct() |> arrange(desc(jw))
# goldilocks value of 0.34 seems to be about right.
inspections |> filter(jw <0.5 & jw > 0.2) |> select(dba_name, ref_dba_name, jw) |> distinct() |> arrange(jw)

```

At this point, we have a good sense of the data and can move on from ad hoc exploration into a repeatable workflow 🎉