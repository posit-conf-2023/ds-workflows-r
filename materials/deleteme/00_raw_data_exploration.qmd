---
title: "00_Raw Data Exploration"
description: "Conduct ad hoc exploration of the data, first exposure to `pointblank`"
toc: true
editor_options: 
  chunk_output_type: console
execute: 
  eval: false
---

This workshop will use data from the City of Chicago Open Data Portal: <https://data.cityofchicago.org>. The following datasets will be used:

1.  üçïFood inspections: <https://data.cityofchicago.org/Health-Human-Services/Food-Inspections/4ijn-s7e5>
2.  üìíBusiness license data: <https://data.cityofchicago.org/Community-Economic-Development/Business-Licenses/r5kz-chrr>

## Setup

Load required packages for importing and exploring the data

```{r}
#| label: Load packages

library(tidyverse)
library(pointblank)

```

## Activity 1 - read data
üîÑ Task

- Explore the two data sources.
- Can you load the data into tibbles (data frames)?

‚úÖ Solution

The City of Chicago data portal provides access to their datasets via the Socrata Open Data (SODA) API:

> The Socrata Open Data API (SODA) provides programmatic access to this dataset including the ability to filter, query, and aggregate data.
=~
Consulting the documentation provides us with some examples of how to use SODA: <https://dev.socrata.com/foundry/data.cityofchicago.org/4ijn-s7e5>.

There is a R package called `RSocrata` to interact with SODA, however we can also query the SODA API directly and download the full dataset as a `.csv` using `readr::read_csv()`. In our initial testing, the column schemas were better preserved using the latter method as opposed to using the `RSocrata` package.



## Download data from City of Chicago

### DATASET 1: Food inspections data

```{r}
#| label: Pull raw inspections data

inspections_raw <- 
  readr::read_csv("https://data.cityofchicago.org/api/views/4ijn-s7e5/rows.csv") 
```



### DATASET 2: Business data

```{r}
#| label: Pull raw business data

bus_data_raw <- 
  readr::read_csv("https://data.cityofchicago.org/api/views/r5kz-chrr/rows.csv") 
```

## Inspect the data

::: callout-tip
## The `pointblank` package

::: columns
::: {.column width="20%"}
![](https://rich-iannone.github.io/pointblank/logo.svg){style="padding-right: 15px;"}
:::

::: {.column width="75%"}
<br> `pointblank` provides data quality assessment and metadata reporting for data frames and database tables.

**Reference**: <https://github.com/rich-iannone/pointblank>
:::
:::
:::

The `pointblank::scan_data()` function provides a HTML report of the input data to help you understand your data. It contains 6 sections:

-   **Overview**: Table dimensions, duplicate row counts, column types, and reproducibility information
-   **Variables**: A summary for each table variable and further statistics and summaries depending on the variable type
-   **Interactions**: A matrix plot that shows interactions between variables
-   **Correlations**: A set of correlation matrix plots for numerical variables
-   **Missing Values**: A summary figure that shows the degree of missingness across variables
-   **Sample**: A table that provides the head and tail rows of the dataset

The scan can take a little while to run on a large dataset, but you can also omit sections that are not needed. 



```{r}
#| label: Data scan

# The pointblank::scan_data function will give us a lovely overview of the data. It takes a little while to run, particularly the Variables and Interactions sections. See this simple example:
pointblank::scan_data(mtcars)


# Our inspection data is much larger than mtcars so to save time, and because these sections aren't relevant for this data, we omit "Correlations" and "Interactions."  
scan <- pointblank::scan_data(inspections_raw, sections = "OVMS")
scan



```

Explore the data scan.

1.  What is the most common value for `dba_name` and `AKA Name` (hint: Toggle details)
2.  Explore the `facility_type`. What values should we consider filtering out or preserving?
3.  What can we infer about the data from the Missing Values diagram?

Conclusions: We've got some very messy data, particularly in dba_name and facility_type. We are going to want the facility_type to be as clean as possible so we can filter out the establishments we don't care about for this analysis (e.g., hotels, caterers, etc.) For our model, we will need the dba_name to be as clean and consistent as possible, so we should do some work here too.

Let's to a first pass thin out the data based on facility_type

```{r}
#| label: thin out data

# When we inspect the Facility Type, we see there is a lot of variation in how the inspector recorded this data. It certainly doesn't align with the stated possible values in the data documentation. For example: 
inspections_raw |> filter(grepl("REST", facility_type)) |> group_by(facility_type) |> tally() |> arrange(desc(n))
inspections_raw |> filter(grepl("BAKERY", facility_type)) |> group_by(facility_type) |> tally() |> arrange(desc(n))
inspections_raw |> filter(grepl("COFFEE", facility_type)) |> group_by(facility_type) |> tally() |> arrange(desc(n))

# We'd like to work with the following types: Restaurant, Bakery, Coffee Shop. And The Wrigley Field Rooftop sounds like a blast.  We'll filter the Facility Type based on regex to account for abbreviations, mis-spellings, and extra information. This is a coarse filter on the raw data before validation to thin out the dataset.  

inspections_pass1 <- 
  inspections_raw |> 
  filter(
    # Filter for "RESTAURANTS" and variations including "REST", "RSTRNT", etc.
    grepl("RE?STA?U?R?A?N?T?", `facility_type`) | 
    # OR Filter for "BAKERY" and variations to account for typos or shortenings
    grepl("BA?KE?R?Y?", `facility_type`) | 
    # OR Filter for "COFFEE SHOP" and variations, but consciously including "SHOP" to avoid carts, roasters, vending machines
    grepl("COFFEE *SHOP", `facility_type`) | 
    # OR Filter for "WRIGLEY"
    grepl("WRIGLEY", `facility_type`)
    ) 

#Now let's also join in the reference business data table so we have a "source of truth" DBA name to check against.
inspections_pass2 <- 
  inspections_pass1 |> 
  left_join(select(bus_data_ref, license_number, doing_business_as_name), by=join_by(license_number)) |>
  rename(ref_dba_name = doing_business_as_name) 

# Finally, add a column with the Jaro ("jw") string distance to help us determine how similar or dissimilar the inspectors' inputted dba name is to the reference. A distance of 0 indicates identical; a 1 indicates completely dissimilar
inspections <- tidystringdist::tidy_stringdist(inspections_pass2,v1=dba_name, v2=ref_dba_name, method="jw") 

# inspecting the data, we can select a jw value of 0.34 as a reasonable threshold for identifying dissimilar values. Take a look:
hist(inspections$jw, breaks = c(seq(0,1,0.05)))
# these are definitely very different
inspections |> filter(jw > 0.5) |> select(dba_name, ref_dba_name, jw) |> distinct() |> arrange(jw)
# these are generally the same
inspections |> filter(jw < 0.2) |> select(dba_name, ref_dba_name, jw) |> distinct() |> arrange(desc(jw))
# goldilocks value of 0.34 seems to be about right.
inspections |> filter(jw <0.5 & jw > 0.2) |> select(dba_name, ref_dba_name, jw) |> distinct() |> arrange(jw)

```

At this point, we have a good sense of the data and can move on from ad hoc exploration into a repeatable workflow üéâ
